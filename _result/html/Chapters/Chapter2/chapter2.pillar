!! Polyglot - Packages and Examples

!!! Installation
To install Polyglot, go to the Playground (Ctrl\+O\+W) in your fresh Pharo image and execute the following Metacello script (select it and press Do-it button or Ctrl\+D):

[[[language=smalltalk
Metacello new
  baseline: 'Polyglot';
  repository: 'github://PolyMathOrg/Polyglot/src';
  load.
]]]

In all keyboard shortcuts mentioned in this booklet the ''Ctrl'' key is for Windows and Linux. On Mac OS, use ''Cmd'' instead.

!!!! Running the tests

First thing you should do after installing Polyglot is open the Polyglot-Tests package in Test Runner (Ctrl\+O\+U) or System Browser (Ctrl\+O\+B) and make sure that all tests are passing. Polyglot is tested with around 100 unit tests which provide 90\% code coverage. If you see some failing tests, please go to the Polyglot repository on GitHub and open a related issue.

!!! PGTokenizer

''PGTokenizer'' is the tokenization object class in ''Polyglot''.

!!!! isSpecial: 

The ''isSpecial:'' method returns ''True'' or ''False'' depending on whether a particular character is a special character or not.

==Usage== 
[[[
|tokenizer|
tokenizer := PGTokenizer new.
tokenizer isSpecial: $?
]]]
should return ''True''.

!!!! sentenceTokenize:

Given a text body, ''sentenceTokenize:'' will return an array of sentences from the text.


==Usage==
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer sentenceTokenize: text.
]]]
should return,
==#('This is a test sentence?' ' Use this as a sample for tokenization')==

!!!! wordTokenize:

Given a text body, ''wordTokenize:'' will return an array of words from the text.

==Usage==
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer wordTokenize: text.
]]]
should return,
==#('This' 'is' 'a' 'test' 'sentence' $? 'Use' 'this' 'as' 'a' 'sample' 'for' 'tokenization')==.

!!!! tokenize:

Given a text body, ''tokenize:'' will return an array of sentences which will further be tokenized into words, that is an array of arrays.

The ''nth'' array would have the tokens of the ''nth'' sentence of the text.

==Usage== 
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer tokenize: text.
]]]
should return,
==#(#('This' 'is' 'a' 'test' 'sentence' $?) #('Use' 'this' 'as' 'a' 'sample' 'for' 'tokenization'))==.

!!!! tokenizeFlatten:

This method is used to return an array of tokens all in a single array instead of an array of array with the ''nth'' array holding the tokens of the ''nth'' sentence of the text. (Utilizes the ''wordTokenize:'' method)

==Usage== 
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer tokenizeFlatten: text.
]]]
should return,
==#('This' 'is' 'a' 'test' 'sentence' $? 'Use' 'this' 'as' 'a' 'sample' 'for' 'tokenization')==.



!!! PGTF-IDF

''PGTF-IDF'' contains a ''PGTfIdf'' object class. This class allows us to directly compute TF-IDF matrices.

Direct usage is as follows,
[[[
|documents tfidf words t|
documents := #(
  (I am Sam)
  (Sam I am)
  (I 'don''t' like green eggs and ham)).
  
tfidf := PGTfIdf new.
tfidf corpus: documents.
]]]

This will return a ''DataFrame'' of term and tfidf scores per each document.

| # | !Sam | !like | !am | !don't | !and | !green | !eggs | !I | !ham
| 1 | 0.058697086351893746 | 0.0 | 0.058697086351893746 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0
| 2 | 0.058697086351893746 | 0.0 | 0.058697086351893746 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0
| 3 | 0.0 | 0.06816017924566606 | 0.0 | 0.06816017924566606 | 0.06816017924566606 | 0.06816017924566606 | 0.06816017924566606 | 0.0 | 0.06816017924566606


!!!! corpus: 
Allows user to send the corpus that they want to use to the ''PGTfIdf'' class (each document should be an array and corpus to be passed as an array of arrays).
==Usage==
[[[
|documents tfidf words t|
documents := #(
  (I am Sam)
  (Sam I am)
  (I 'don''t' like green eggs and ham)).
  
tfidf := PGTfIdf new.
tfidf corpus: documents.
]]] 

==Result==
| # | !Sam | !like | !am | !don't | !and | !green | !eggs | !I | !ham
| 1 | 0.058697086351893746 | 0.0 | 0.058697086351893746 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0
| 2 | 0.058697086351893746 | 0.0 | 0.058697086351893746 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0
| 3 | 0.0 | 0.06816017924566606 | 0.0 | 0.06816017924566606 | 0.06816017924566606 | 0.06816017924566606 | 0.06816017924566606 | 0.0 | 0.06816017924566606

!!!! corpus
Allows user to get the corpus used to compute Tf-Idf scores.

==Usage==
[[[
|documents tfidf words t|
documents := #(
  (I am Sam)
  (Sam I am)
  (I 'don''t' like green eggs and ham)).
  
tfidf := PGTfIdf new.
tfidf corpus: documents.
]]] 

The return should be the same as the ''documents'' object in the code snippet above, that is,
[[[
  #( (I am Sam) (Sam I am) (I 'don''t' like green eggs and ham)).
]]]
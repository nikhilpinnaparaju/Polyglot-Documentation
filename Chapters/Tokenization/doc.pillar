!! Package - PGTokenizer

!!! Class - PGTokenizer

''PGTokenizer'' is the tokenization object class in ''Polyglot''.

!!!! isSpecial: 

The ''isSpecial:'' method returns ''True'' or ''False'' depending on whether a particular character is a special character or not.

==Usage== 
[[[
|tokenizer|
tokenizer := PGTokenizer new.
tokenizer isSpecial: $?
]]]
should return ''True''.

!!!! sentenceTokenize:

Given a text body, ''sentenceTokenize:'' will return an array of sentences from the text.


==Usage==
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer sentenceTokenize: text.
]]]
should return,
==#('This is a test sentence?' ' Use this as a sample for tokenization')==

!!!! wordTokenize:

Given a text body, ''wordTokenize:'' will return an array of words from the text.

==Usage==
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer wordTokenize: text.
]]]
should return,
==#('This' 'is' 'a' 'test' 'sentence' $? 'Use' 'this' 'as' 'a' 'sample' 'for' 'tokenization')==.

!!!! tokenize:

Given a text body, ''tokenize:'' will return an array of sentences which will further be tokenized into words, that is an array of arrays.

The ''nth'' array would have the tokens of the ''nth'' sentence of the text.

==Usage== 
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer tokenize: text.
]]]
should return,
==#(#('This' 'is' 'a' 'test' 'sentence' $?) #('Use' 'this' 'as' 'a' 'sample' 'for' 'tokenization'))==.

!!!! tokenizeFlatten:

This method is used to return an array of tokens all in a single array instead of an array of array with the ''nth'' array holding the tokens of the ''nth'' sentence of the text. (Utilizes the ''wordTokenize:'' method)

==Usage== 
[[[
|tokenizer text|
text := 'This is a test sentence? Use this as a sample for tokenization'.
tokenizer := PGTokenizer new.
tokenizer tokenizeFlatten: text.
]]]
should return,
==#('This' 'is' 'a' 'test' 'sentence' $? 'Use' 'this' 'as' 'a' 'sample' 'for' 'tokenization')==.